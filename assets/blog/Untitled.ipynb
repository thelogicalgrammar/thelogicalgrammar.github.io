{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d87c01e-b584-43ab-9656-2270c6d6e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, genpareto\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e5a68-c0fc-4a32-980d-e205b1d2ab6b",
   "metadata": {},
   "source": [
    "\n",
    "Let's start with equation 6 in Vehtari (2017):\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{p(\\theta^s \\mid y_{-i})}{p(\\theta^s \\mid y)}\n",
    "&= \\frac{\n",
    "    \\frac{p( y_{-i} \\mid \\theta^s) p(\\theta^s)}{p(y_{-i})} \n",
    "}{\n",
    "    \\frac{p( y \\mid \\theta^s) p(\\theta^s)}{p(y)} \n",
    "} & \\text{Apply Bayes to nom and denom} \\\\\n",
    "&= \\frac{\n",
    "    \\frac{p( y_{-i} \\mid \\theta^s) p(\\theta^s)}{p(y_{-i})} \n",
    "}{\n",
    "    \\frac{p( y_{-i} \\mid \\theta^s) p( y_{i} \\mid \\theta^s) p(\\theta^s)}{p(y_{i})} \n",
    "} & \\text{Decompose $y$ (assumes independence)} \\\\\n",
    "&= \\frac{\n",
    "    \\frac{1}{p(y_{-i})} \n",
    "}{\n",
    "    \\frac{p( y_{i} \\mid \\theta^s)}{p(y_{i})} \n",
    "} & \\text{Simplify} \\\\\n",
    "&= \\frac{p(y_{i})}{p(y_{-i})}\\frac{1}{p( y_{i} \\mid \\theta^s)} & \\text{Rewrite} \\\\\n",
    "&\\propto \\frac{1}{p( y_{i} \\mid \\theta^s)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "So in short we get Eq. 6 in Vehtari et al (2017):\n",
    "\n",
    "\\begin{align*}\n",
    "r^s_i \n",
    "&= \\frac{1}{p(y_i \\mid \\theta^s)} \\\\\n",
    "&\\propto \\frac{p(\\theta^s \\mid y_{-i})}{p(\\theta^s \\mid y)}\n",
    "\\end{align*}\n",
    "\n",
    "From these weights, we can get an approximation of:\n",
    "\n",
    "\\begin{align*}\n",
    "p( y_i \\mid y_{-i} )\n",
    "&= \\frac{p( y_{-i}, y_i) }{p(y_{-i})} & \\text{Def of conditional prob} \\\\\n",
    "&= \\frac{p(y)}{p(y_{-i})} & \\text{Page 154 of Gelfand (1996)}\n",
    "\\end{align*}\n",
    "\n",
    "Let's first consider not the removed datapoint, but \\textit{some} datapoint $\\tilde{y}_i$ given the dataset without datapoint $y_i$. The trick that we use to approximate is the \\textit{self normalizing importance sampling},\\footnote{Derivation adapted from \\url{https://www.math.arizona.edu/~tgk/mc/book_chap6.pdf}} which we can use whenever we have the samples instead of the closed form:\n",
    "\n",
    "\\begin{align*}\\label{eq:continuousImportanceSampling}\n",
    "p(\\tilde{y}_i \\mid y_{-i} )& = \\int p(\\tilde{y}_i \\mid \\theta) p(\\theta \\mid y_{-i})\n",
    "\\\\\n",
    "& =\n",
    "\\int p(\\tilde{y}_i \\mid \\theta) \\frac{p(\\theta \\mid y_{-i})}{p(\\theta \\mid y)} p(\\theta \\mid y) d\\theta \\\\\n",
    "& = \n",
    "\\frac{\\int p(\\tilde{y}_i \\mid \\theta) \\frac{p(\\theta \\mid y_{-i})}{p(\\theta \\mid y)} p(\\theta \\mid y) d\\theta}\n",
    "{\\int \\frac{p(\\theta \\mid y_{-i})}{p(\\theta \\mid y)} p(\\theta \\mid y)d\\theta}\n",
    "& \\text{Denominator integrates to 1}\n",
    "\\\\\n",
    "& = \n",
    "\\frac{\\int p(\\tilde{y}_i \\mid \\theta) c r_i^\\theta p(\\theta \\mid y)  d\\theta}\n",
    "{\\int c r_i^\\theta p(\\theta \\mid y) d\\theta} \\\\\n",
    "& = \n",
    "\\frac{c \\int p(\\tilde{y}_i \\mid \\theta) r_i^\\theta p(\\theta \\mid y)  d\\theta}\n",
    "{c \\int r_i^\\theta p(\\theta \\mid y) d\\theta} \\\\\n",
    "& = \n",
    "\\frac{\\int p(\\tilde{y}_i \\mid \\theta) r_i^\\theta p(\\theta \\mid y)  d\\theta}\n",
    "{\\int r_i^\\theta p(\\theta \\mid y) d\\theta} \\\\\n",
    "& =\n",
    "\\frac{\\mathbb{E}_{p(\\theta \\mid y)} \\left[ p(\\tilde{y}_i \\mid \\theta) r_i^\\theta \\right]}\n",
    "{\\mathbb{E}_{p(\\theta \\mid y)} \\left[ r_i^\\theta \\right]}\n",
    "\\end{align*}\n",
    "\n",
    "where $r_i^\\theta = \\frac{p(\\theta \\mid y_{-i})}{p(\\theta \\mid y)}$ and $c=\\frac{p(y_{i})}{p(y_{-i})}$. Since we don't have the actual density function, but rather just samples, we approximate the integrals with:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\mathbb{E}_{p(\\theta \\mid y)} \\left[ p(\\tilde{y}_i \\mid \\theta) r_i^\\theta \\right]}\n",
    "{\\mathbb{E}_{p(\\theta \\mid y)} \\left[ r_i^\\theta \\right]}\n",
    "&\\approx\n",
    "\\frac\n",
    "{\\sum_s p(\\tilde{y}_i \\mid \\theta^s) r_i^s}\n",
    "{\\sum_s r_i^s}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3de7916-6a82-47c7-a343-3524f6b81fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d73a45-878c-43b2-9d33-8a57c3be2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(x, loc, scale=sigma):\n",
    "    return norm.pdf(x=x, loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab123cfb-2b99-4912-9957-611650e2c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_ratio(y, posterior_samples, i, s):\n",
    "    return compute_likelihood(y[i], posterior_samples[s])**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf2f9c5-46e9-49bf-922c-0a21b42af8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = norm(loc=4, scale=1).rvs(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b441404f-a232-485c-81f8-1e98c00d79f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.57221184, 5.45600575, 5.12237827, 4.84868742, 4.13163957,\n",
       "       2.76809091, 3.74048448, 3.36317861, 3.32984346, 4.51422281])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9283130-cad9-408f-af70-4c48ad89c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal(\n",
    "        'mu',\n",
    "        mu=0,\n",
    "        sigma=3\n",
    "    )\n",
    "    \n",
    "    pm.Normal(\n",
    "        'y',\n",
    "        mu=mu,\n",
    "        sigma=sigma,\n",
    "        observed=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74705f2-61be-4e01-b407-64c7b21cff5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    trace = pm.sample(\n",
    "        return_inferencedata=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84372780-8977-465d-9687-c76bf1d9d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = trace.posterior.mu.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb69529-fe69-4b84-a320-6bf5bbb31589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f594cc69190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f0cd21-f066-4ddf-a035-d2c499c7492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.array([\n",
    "    [\n",
    "        # calculate the important ratio \n",
    "        importance_ratio(y, posterior_samples, i, s) \n",
    "        # for each datapoint\n",
    "        for i in range(len(y))\n",
    "    ]\n",
    "    # for each posterior sample\n",
    "    for s in range(len(posterior_samples))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c72956-1dd6-406c-9c87-13ed7779af91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.04983215, 8.81548776, 7.48715185, ..., 5.01390306, 5.01325748,\n",
       "        5.97192944],\n",
       "       [5.03183284, 8.50265774, 7.26322344, ..., 5.01411543, 5.01635933,\n",
       "        5.85453786],\n",
       "       [5.02403665, 8.32742588, 7.13785851, ..., 5.01706522, 5.02102264,\n",
       "        5.78939855],\n",
       "       ...,\n",
       "       [5.43900544, 5.79434333, 5.37106759, ..., 5.70451271, 5.75384418,\n",
       "        5.0246047 ],\n",
       "       [5.31049227, 6.01094703, 5.51235674, ..., 5.53239134, 5.57425234,\n",
       "        5.05685471],\n",
       "       [5.02114193, 8.24865569, 7.08152871, ..., 5.01914434, 5.02389704,\n",
       "        5.76028886]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77a9647-5d06-4da4-8502-26a3fe51063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IS_LOO_LPD(y_squiggle, rs, posterior_samples):\n",
    "    # Eq. 7\n",
    "    return (\n",
    "        (\n",
    "            compute_likelihood(\n",
    "                y_squiggle, \n",
    "                posterior_samples[:,None], \n",
    "            )*rs\n",
    "        ).sum(0) \n",
    "        / \n",
    "        rs.sum(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62ac98d-935a-459d-92f0-38d625c93090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02579605, 0.02517676, 0.01200173, 0.01481117, 0.05738864,\n",
       "       0.02587117, 0.02813902, 0.02355415, 0.09967979, 0.04344011])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_squiggle = np.random.normal(size=10)\n",
    "\n",
    "# approximated posterior probability of y_squiggle \n",
    "# conditioned on LOO folds\n",
    "IS_LOO_LPD(\n",
    "    y_squiggle, \n",
    "    rs, \n",
    "    posterior_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abb3206-85c1-4807-a898-173326392cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lik_yi_given_thetas(y, posterior_samples):\n",
    "     return compute_likelihood(\n",
    "        y, \n",
    "        # add dimension to make it broadcastable\n",
    "        posterior_samples[:,None], \n",
    "        scale=1\n",
    "    )\n",
    "\n",
    "def harmonic_mean(y, posterior_samples):\n",
    "    # Eq. 8\n",
    "    \n",
    "    # Dims: (sample, datapoint)\n",
    "    lik_yi_given_thetas = calculate_lik_yi_given_thetas(\n",
    "        y,\n",
    "        posterior_samples\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        len(posterior_samples)**-1 \n",
    "        * (lik_yi_given_thetas**-1).sum(0)\n",
    "    )**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad0957d6-d889-4eb1-98ba-26a9bd478652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the harmonic mean is close to\n",
    "# IS_LOO_LPD calculated at left out points\n",
    "# As stated in Eq. 8\n",
    "np.isclose(\n",
    "    harmonic_mean(y, posterior_samples), \n",
    "    IS_LOO_LPD(y, rs, posterior_samples)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3426fc2b-fffb-44c5-a7ca-eae4efb621a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpdfit(ary):\n",
    "    \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
    "\n",
    "    Empirical Bayes estimate for the parameters of the generalized Pareto\n",
    "    distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary: array\n",
    "        sorted 1D data array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k: float\n",
    "        estimated shape parameter\n",
    "    sigma: float\n",
    "        estimated scale parameter\n",
    "    \"\"\"\n",
    "    prior_bs = 3\n",
    "    prior_k = 10\n",
    "    n = len(ary)\n",
    "    m_est = 30 + int(n ** 0.5)\n",
    "\n",
    "    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
    "    b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
    "    b_ary += 1 / ary[-1]\n",
    "\n",
    "    k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
    "    len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
    "    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
    "\n",
    "    # remove negligible weights\n",
    "    real_idxs = weights >= 10 * np.finfo(float).eps\n",
    "    if not np.all(real_idxs):\n",
    "        weights = weights[real_idxs]\n",
    "        b_ary = b_ary[real_idxs]\n",
    "    # normalise weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b_post = np.sum(b_ary * weights)\n",
    "    # estimate for k\n",
    "    k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
    "    # add prior for k_post\n",
    "    k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
    "    sigma = -k_post / b_post\n",
    "\n",
    "    return k_post, sigma\n",
    "\n",
    "\n",
    "def _gpinv(probs, kappa, sigma):\n",
    "    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
    "    # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
    "    x = np.full_like(probs, np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (probs > 0) & (probs < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x = -np.log1p(-probs)\n",
    "        else:\n",
    "            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x[ok] = -np.log1p(-probs[ok])\n",
    "        else:\n",
    "            x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
    "        x *= sigma\n",
    "        x[probs == 0] = 0\n",
    "        if kappa >= 0:\n",
    "            x[probs == 1] = np.inf\n",
    "        else:\n",
    "            x[probs == 1] = -sigma / kappa\n",
    "    return x\n",
    "\n",
    "\n",
    "def _psislw(log_weights, cutoff_ind, cutoffmin, k_min=1.0 / 3):\n",
    "    \"\"\"\n",
    "    Pareto smoothed importance sampling (PSIS) for a 1D vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_weights: array\n",
    "        Array of length n_observations\n",
    "    cutoff_ind: int\n",
    "    cutoffmin: float\n",
    "    k_min: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out: array\n",
    "        Smoothed log weights\n",
    "    kss: float\n",
    "        Pareto tail index\n",
    "    \"\"\"\n",
    "    x = np.asarray(log_weights)\n",
    "\n",
    "    # improve numerical accuracy\n",
    "    x -= np.max(x)\n",
    "    # sort the array\n",
    "    x_sort_ind = np.argsort(x)\n",
    "    # divide log weights into body and right tail\n",
    "    xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
    "\n",
    "    expxcutoff = np.exp(xcutoff)\n",
    "    (tailinds,) = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
    "    x_tail = x[tailinds]\n",
    "    tail_len = len(x_tail)\n",
    "    if tail_len <= 4:\n",
    "        # not enough tail samples for gpdfit\n",
    "        k = np.inf\n",
    "    else:\n",
    "        # order of tail samples\n",
    "        x_tail_si = np.argsort(x_tail)\n",
    "        # fit generalized Pareto distribution to the right tail samples\n",
    "        x_tail = np.exp(x_tail) - expxcutoff\n",
    "        k, sigma = _gpdfit(x_tail[x_tail_si])\n",
    "\n",
    "        if k >= k_min:\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, tail_len) / tail_len\n",
    "            smoothed_tail = _gpinv(sti, k, sigma)\n",
    "            smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
    "                smoothed_tail + expxcutoff\n",
    "            )\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x_tail_si]] = smoothed_tail\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "    # renormalize weights\n",
    "    x -= _logsumexp(x)\n",
    "\n",
    "    return x, k\n",
    "\n",
    "\n",
    "def psislw(log_weights, reff=1.0):\n",
    "    \"\"\"\n",
    "    Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If the ``log_weights`` input is an :class:`~xarray.DataArray` with a dimension\n",
    "    named ``__sample__`` (recommended) ``psislw`` will interpret this dimension as samples,\n",
    "    and all other dimensions as dimensions of the observed data, looping over them to\n",
    "    calculate the psislw of each observation. If no ``__sample__`` dimension is present or\n",
    "    the input is a numpy array, the last dimension will be interpreted as ``__sample__``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_weights: array\n",
    "        Array of size (n_observations, n_samples)\n",
    "    reff: float\n",
    "        relative MCMC efficiency, `ess / n`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out: array\n",
    "        Smoothed log weights\n",
    "    kss: array\n",
    "        Pareto tail indices\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    * Vehtari et al. (2015) see https://arxiv.org/abs/1507.02646\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Get Pareto smoothed importance sampling (PSIS) log weights:\n",
    "\n",
    "    .. ipython::\n",
    "\n",
    "        In [1]: import arviz as az\n",
    "           ...: data = az.load_arviz_data(\"centered_eight\")\n",
    "           ...: log_likelihood = data.sample_stats.log_likelihood.stack(\n",
    "           ...:     __sample__=(\"chain\", \"draw\")\n",
    "           ...: )\n",
    "           ...: az.psislw(-log_likelihood, reff=0.8)\n",
    "\n",
    "    \"\"\"\n",
    "    if hasattr(log_weights, \"__sample__\"):\n",
    "        n_samples = len(log_weights.__sample__)\n",
    "        shape = [\n",
    "            size for size, dim in zip(log_weights.shape, log_weights.dims) if dim != \"__sample__\"\n",
    "        ]\n",
    "    else:\n",
    "        n_samples = log_weights.shape[-1]\n",
    "        shape = log_weights.shape[:-1]\n",
    "    # precalculate constants\n",
    "    cutoff_ind = -int(np.ceil(min(n_samples / 5.0, 3 * (n_samples / reff) ** 0.5))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
    "    k_min = 1.0 / 3\n",
    "\n",
    "    # create output array with proper dimensions\n",
    "    out = tuple([np.empty_like(log_weights), np.empty(shape)])\n",
    "\n",
    "    # define kwargs\n",
    "    func_kwargs = {\"cutoff_ind\": cutoff_ind, \"cutoffmin\": cutoffmin, \"k_min\": k_min, \"out\": out}\n",
    "    ufunc_kwargs = {\"n_dims\": 1, \"n_output\": 2, \"ravel\": False, \"check_shape\": False}\n",
    "    kwargs = {\"input_core_dims\": [[\"__sample__\"]], \"output_core_dims\": [[\"__sample__\"], []]}\n",
    "    log_weights, pareto_shape = _wrap_xarray_ufunc(\n",
    "        _psislw,\n",
    "        log_weights,\n",
    "        ufunc_kwargs=ufunc_kwargs,\n",
    "        func_kwargs=func_kwargs,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return log_weights, pareto_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47d6845e-2ea5-409c-b4e9-693b627626fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arviz.stats.stats_utils import logsumexp as _logsumexp\n",
    "from arviz.stats.stats_utils import wrap_xarray_ufunc as _wrap_xarray_ufunc\n",
    "from arviz.stats.diagnostics import ess\n",
    "\n",
    "# precalculate constants\n",
    "n_samples = len(rs)\n",
    "reff = 1.0\n",
    "cutoff_ind = -int(np.ceil(min(n_samples / 5.0, 3 * (n_samples / reff) ** 0.5))) - 1\n",
    "cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
    "k_min = 1.0 / 3\n",
    "\n",
    "ess_p = ess(trace.posterior.mu, method=\"mean\")\n",
    "# this mean is over all data variables\n",
    "reff = (\n",
    "    np.hstack([ess_p[v].values.flatten() for v in ess_p.data_vars]).mean() / n_samples\n",
    ")\n",
    "\n",
    "\n",
    "log_weights, pareto_shape = psislw(\n",
    "    -logliks, \n",
    "    reff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cfa075a-feac-4644-8f03-cadbd11a62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n observations, n samples)\n",
    "logliks = compute_likelihood(\n",
    "    y[:,None,None,None], \n",
    "    trace.posterior.mu.values, \n",
    "    scale=sigma\n",
    ").reshape(10, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdb6273c-ed62-40b7-9942-5da917a42901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logliks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea870947-1bcd-4633-b532-8f64b266a2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-8.30439242, -8.30510078, -8.30540917, ..., -8.29022318,\n",
       "        -8.29467249, -8.30552392]),\n",
       " -0.11155126223423353)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_psislw(\n",
    "    -logliks[0],\n",
    "    cutoff_ind,\n",
    "    cutoffmin,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04ae3557-b13f-45cd-88db-4e565c47af1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07079134, -0.12844772, -0.09451548, -0.06449457,  0.10014695,\n",
       "       -0.14955973,  0.00869916, -0.08565987, -0.08695632, -0.02328076])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pareto_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2fbd417-9773-4232-9dc7-bdba1e795a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.86059767, -0.30705933, -0.36256923, -0.41643817, -0.62460325,\n",
       "       -0.59766199, -0.90644191, -0.77301169, -0.76095734, -0.49580881])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ddebbaf-8c7b-4864-be1b-a54e67396d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "rs_sorted = np.sort(rs, axis=0)\n",
    "\n",
    "# Get bottom and top 20% ratios\n",
    "# for each fold independently\n",
    "rs_bottom80, rs_top20 = np.split(\n",
    "    # sort rs values\n",
    "    # (independently for each datapoint)\n",
    "    rs_sorted, \n",
    "    # split bottom 80% and top 20%\n",
    "    [int(len(rs_sorted)*0.8)],\n",
    "    # along axis 0\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f0ddb82-d4e2-436c-8110-06df8e0d71f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_871/1645870637.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Fit a pareto on the 20% largest importance ratios\n",
    "# For each fold independently\n",
    "ks, sigmas = np.apply_along_axis(\n",
    "    # returns fitted (k, sigma) params\n",
    "    _gpdfit,\n",
    "    # run for each datapoint\n",
    "    axis=0, \n",
    "    # on top 20% weights\n",
    "    arr=rs_top20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddf854b1-7b9b-4d27-8530-a1e21663176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "# Stabilize the top 20% weights\n",
    "\n",
    "M = int(len(rs)*0.2)\n",
    "stabilized_top20 = []\n",
    "# loop over folds\n",
    "for i, rs_i in enumerate(rs_sorted.T):\n",
    "        # calculate inverse CDF of the generalized Pareto\n",
    "        # for that datapoint\n",
    "        stabilized_top20.append(_gpinv(\n",
    "            (\n",
    "                np.arange(0.5,M)\n",
    "                /\n",
    "                (len(rs_sorted)*0.2)\n",
    "            ).T,\n",
    "            ks[i], \n",
    "            sigmas[i]\n",
    "        ))\n",
    "stabilized_top20 = np.array(stabilized_top20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be23a42b-fa71-4c5a-98ba-d5dcb1195f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stabilized_top20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02640a26-55ab-405d-8a89-fa5cde7bc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3\n",
    "\n",
    "# First, recombine the two\n",
    "stabilized_rs = np.concatenate((\n",
    "    rs_bottom80,\n",
    "    stabilized_top20\n",
    "))\n",
    "\n",
    "# calculate truncation values\n",
    "# for each fold independently\n",
    "trunc = (\n",
    "    (\n",
    "        len(stabilized_rs)**(3/4)\n",
    "    )\n",
    "    *stabilized_rs.mean(0,keepdims=True)\n",
    ")\n",
    "\n",
    "# finally, truncate the stabilized weights\n",
    "truncated_rs = np.where(\n",
    "    stabilized_rs < trunc,\n",
    "    stabilized_rs,\n",
    "    trunc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "386e6e01-ea86-4076-8b3e-04842fd9fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lik_ys_given_thetas = calculate_lik_yi_given_thetas(\n",
    "    y, \n",
    "    posterior_samples\n",
    ")\n",
    "\n",
    "# Eq. 10\n",
    "ELPD_PSIS_LOO = np.log(\n",
    "    (truncated_rs*lik_ys_given_thetas).sum(0)\n",
    "    /\n",
    "    truncated_rs.sum(0)\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c763ae6-17c3-4a52-8446-75ad38a0abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.405006735744953"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELPD_PSIS_LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a91921a-5908-4a40-bcbf-df0aa880685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmloo = pm.loo(trace, pointwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d2639d5-0e6a-4adc-a411-2ab5d6a07ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;pareto_shape&#x27; (y_dim_0: 10)&gt;\n",
       "array([0.12010851, 0.23146598, 0.22189066, 0.21654693, 0.288042  ,\n",
       "       0.14137858, 0.19395604, 0.13047512, 0.13399558, 0.21512154])\n",
       "Coordinates:\n",
       "  * y_dim_0  (y_dim_0) int64 0 1 2 3 4 5 6 7 8 9</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'pareto_shape'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>y_dim_0</span>: 10</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-a30bf084-c0d9-4035-9c3c-adc3e4cb21f3' class='xr-array-in' type='checkbox' checked><label for='section-a30bf084-c0d9-4035-9c3c-adc3e4cb21f3' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.1201 0.2315 0.2219 0.2165 0.288 0.1414 0.194 0.1305 0.134 0.2151</span></div><div class='xr-array-data'><pre>array([0.12010851, 0.23146598, 0.22189066, 0.21654693, 0.288042  ,\n",
       "       0.14137858, 0.19395604, 0.13047512, 0.13399558, 0.21512154])</pre></div></div></li><li class='xr-section-item'><input id='section-bea3f9ea-1caf-4490-857e-ac6e4873efa5' class='xr-section-summary-in' type='checkbox'  checked><label for='section-bea3f9ea-1caf-4490-857e-ac6e4873efa5' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y_dim_0</span></div><div class='xr-var-dims'>(y_dim_0)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 7 8 9</div><input id='attrs-30688109-6d7e-4c9b-82f8-b59f53efe951' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-30688109-6d7e-4c9b-82f8-b59f53efe951' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a40d506e-dcaa-4025-8735-006d865df95e' class='xr-var-data-in' type='checkbox'><label for='data-a40d506e-dcaa-4025-8735-006d865df95e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-da7c02ee-93e9-460f-aaad-d3e9f37ac95c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-da7c02ee-93e9-460f-aaad-d3e9f37ac95c' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'pareto_shape' (y_dim_0: 10)>\n",
       "array([0.12010851, 0.23146598, 0.22189066, 0.21654693, 0.288042  ,\n",
       "       0.14137858, 0.19395604, 0.13047512, 0.13399558, 0.21512154])\n",
       "Coordinates:\n",
       "  * y_dim_0  (y_dim_0) int64 0 1 2 3 4 5 6 7 8 9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmloo.pareto_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c79400-de21-4aa4-a1c3-e5f5ba88cbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.86059767, -0.30705933, -0.36256923, -0.41643817, -0.62460325,\n",
       "       -0.59766199, -0.90644191, -0.77301169, -0.76095734, -0.49580881])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2d3ee542-45ba-4b75-8f6d-cd8afff6d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "72a8c741-d207-4bbd-a8fd-c8f202c4f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def psislw(log_weights, reff=1.0):\n",
      "    \"\"\"\n",
      "    Pareto smoothed importance sampling (PSIS).\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    If the ``log_weights`` input is an :class:`~xarray.DataArray` with a dimension\n",
      "    named ``__sample__`` (recommended) ``psislw`` will interpret this dimension as samples,\n",
      "    and all other dimensions as dimensions of the observed data, looping over them to\n",
      "    calculate the psislw of each observation. If no ``__sample__`` dimension is present or\n",
      "    the input is a numpy array, the last dimension will be interpreted as ``__sample__``.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    log_weights: array\n",
      "        Array of size (n_observations, n_samples)\n",
      "    reff: float\n",
      "        relative MCMC efficiency, ``ess / n``\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    lw_out: array\n",
      "        Smoothed log weights\n",
      "    kss: array\n",
      "        Pareto tail indices\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    * Vehtari et al. (2015) see https://arxiv.org/abs/1507.02646\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Get Pareto smoothed importance sampling (PSIS) log weights:\n",
      "\n",
      "    .. ipython::\n",
      "\n",
      "        In [1]: import arviz as az\n",
      "           ...: data = az.load_arviz_data(\"centered_eight\")\n",
      "           ...: log_likelihood = data.sample_stats.log_likelihood.stack(\n",
      "           ...:     __sample__=(\"chain\", \"draw\")\n",
      "           ...: )\n",
      "           ...: az.psislw(-log_likelihood, reff=0.8)\n",
      "\n",
      "    \"\"\"\n",
      "    if hasattr(log_weights, \"__sample__\"):\n",
      "        n_samples = len(log_weights.__sample__)\n",
      "        shape = [\n",
      "            size for size, dim in zip(log_weights.shape, log_weights.dims) if dim != \"__sample__\"\n",
      "        ]\n",
      "    else:\n",
      "        n_samples = log_weights.shape[-1]\n",
      "        shape = log_weights.shape[:-1]\n",
      "    # precalculate constants\n",
      "    cutoff_ind = -int(np.ceil(min(n_samples / 5.0, 3 * (n_samples / reff) ** 0.5))) - 1\n",
      "    cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
      "\n",
      "    # create output array with proper dimensions\n",
      "    out = tuple([np.empty_like(log_weights), np.empty(shape)])\n",
      "\n",
      "    # define kwargs\n",
      "    func_kwargs = {\"cutoff_ind\": cutoff_ind, \"cutoffmin\": cutoffmin, \"out\": out}\n",
      "    ufunc_kwargs = {\"n_dims\": 1, \"n_output\": 2, \"ravel\": False, \"check_shape\": False}\n",
      "    kwargs = {\"input_core_dims\": [[\"__sample__\"]], \"output_core_dims\": [[\"__sample__\"], []]}\n",
      "    log_weights, pareto_shape = _wrap_xarray_ufunc(\n",
      "        _psislw,\n",
      "        log_weights,\n",
      "        ufunc_kwargs=ufunc_kwargs,\n",
      "        func_kwargs=func_kwargs,\n",
      "        **kwargs,\n",
      "    )\n",
      "    if isinstance(log_weights, xr.DataArray):\n",
      "        log_weights = log_weights.rename(\"log_weights\")\n",
      "    if isinstance(pareto_shape, xr.DataArray):\n",
      "        pareto_shape = pareto_shape.rename(\"pareto_shape\")\n",
      "    return log_weights, pareto_shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(az.psislw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "26848949-2f75-4aac-9dc5-002a7236b006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'arviz.stats.stats_utils' from '/home/fausto/anaconda3/envs/argumentative_language/lib/python3.8/site-packages/arviz/stats/stats_utils.py'>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.stats_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a0ebdef8-3a2d-4f2c-80bc-eab16483c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pylint: disable=invalid-name,too-many-lines\n",
      "\"\"\"Density estimation functions for ArviZ.\"\"\"\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "from scipy.fftpack import fft\n",
      "from scipy.optimize import brentq\n",
      "from scipy.signal import convolve, convolve2d, gaussian  # pylint: disable=no-name-in-module\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.special import ive  # pylint: disable=no-name-in-module\n",
      "\n",
      "from ..utils import _cov, _dot, _stack, conditional_jit\n",
      "\n",
      "__all__ = [\"kde\"]\n",
      "\n",
      "\n",
      "def _bw_scott(x, x_std=None, **kwargs):  # pylint: disable=unused-argument\n",
      "    \"\"\"Scott's Rule.\"\"\"\n",
      "    if x_std is None:\n",
      "        x_std = np.std(x)\n",
      "    bw = 1.06 * x_std * len(x) ** (-0.2)\n",
      "    return bw\n",
      "\n",
      "\n",
      "def _bw_silverman(x, x_std=None, **kwargs):  # pylint: disable=unused-argument\n",
      "    \"\"\"Silverman's Rule.\"\"\"\n",
      "    if x_std is None:\n",
      "        x_std = np.std(x)\n",
      "    q75, q25 = np.percentile(x, [75, 25])\n",
      "    x_iqr = q75 - q25\n",
      "    a = min(x_std, x_iqr / 1.34)\n",
      "    bw = 0.9 * a * len(x) ** (-0.2)\n",
      "    return bw\n",
      "\n",
      "\n",
      "def _bw_isj(x, grid_counts=None, x_std=None, x_range=None):\n",
      "    \"\"\"Improved Sheather-Jones bandwidth estimation.\n",
      "\n",
      "    Improved Sheather and Jones method as explained in [1]_. This method is used internally by the\n",
      "    KDE estimator, resulting in saved computation time as minimums, maximums and the grid are\n",
      "    pre-computed.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Kernel density estimation via diffusion.\n",
      "       Z. I. Botev, J. F. Grotowski, and D. P. Kroese.\n",
      "       Ann. Statist. 38 (2010), no. 5, 2916--2957.\n",
      "    \"\"\"\n",
      "    x_len = len(x)\n",
      "    if x_range is None:\n",
      "        x_min = np.min(x)\n",
      "        x_max = np.max(x)\n",
      "        x_range = x_max - x_min\n",
      "\n",
      "    # Relative frequency per bin\n",
      "    if grid_counts is None:\n",
      "        x_std = np.std(x)\n",
      "        grid_len = 256\n",
      "        grid_min = x_min - 0.5 * x_std\n",
      "        grid_max = x_max + 0.5 * x_std\n",
      "        grid_counts, _, _ = histogram(x, grid_len, (grid_min, grid_max))\n",
      "    else:\n",
      "        grid_len = len(grid_counts) - 1\n",
      "\n",
      "    grid_relfreq = grid_counts / x_len\n",
      "\n",
      "    # Discrete cosine transform of the data\n",
      "    a_k = _dct1d(grid_relfreq)\n",
      "\n",
      "    k_sq = np.arange(1, grid_len) ** 2\n",
      "    a_sq = a_k[range(1, grid_len)] ** 2\n",
      "\n",
      "    t = _root(_fixed_point, x_len, args=(x_len, k_sq, a_sq), x=x)\n",
      "    h = t**0.5 * x_range\n",
      "    return h\n",
      "\n",
      "\n",
      "def _bw_experimental(x, grid_counts=None, x_std=None, x_range=None):\n",
      "    \"\"\"Experimental bandwidth estimator.\"\"\"\n",
      "    bw_silverman = _bw_silverman(x, x_std=x_std)\n",
      "    bw_isj = _bw_isj(x, grid_counts=grid_counts, x_range=x_range)\n",
      "    return 0.5 * (bw_silverman + bw_isj)\n",
      "\n",
      "\n",
      "def _bw_taylor(x):\n",
      "    \"\"\"Taylor's rule for circular bandwidth estimation.\n",
      "\n",
      "    This function implements a rule-of-thumb for choosing the bandwidth of a von Mises kernel\n",
      "    density estimator that assumes the underlying distribution is von Mises as introduced in [1]_.\n",
      "    It is analogous to Scott's rule for the Gaussian KDE.\n",
      "\n",
      "    Circular bandwidth has a different scale from linear bandwidth. Unlike linear scale, low\n",
      "    bandwidths are associated with oversmoothing and high values with undersmoothing.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] C.C Taylor (2008). Automatic bandwidth selection for circular\n",
      "           density estimation.\n",
      "           Computational Statistics and Data Analysis, 52, 7, 3493–3500.\n",
      "    \"\"\"\n",
      "    x_len = len(x)\n",
      "    kappa = _kappa_mle(x)\n",
      "    num = 3 * x_len * kappa**2 * ive(2, 2 * kappa)\n",
      "    den = 4 * np.pi**0.5 * ive(0, kappa) ** 2\n",
      "    return (num / den) ** 0.4\n",
      "\n",
      "\n",
      "_BW_METHODS_LINEAR = {\n",
      "    \"scott\": _bw_scott,\n",
      "    \"silverman\": _bw_silverman,\n",
      "    \"isj\": _bw_isj,\n",
      "    \"experimental\": _bw_experimental,\n",
      "}\n",
      "\n",
      "\n",
      "def _get_bw(x, bw, grid_counts=None, x_std=None, x_range=None):\n",
      "    \"\"\"Compute bandwidth for a given data `x` and `bw`.\n",
      "\n",
      "    Also checks `bw` is correctly specified.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : 1-D numpy array\n",
      "        1 dimensional array of sample data from the\n",
      "        variable for which a density estimate is desired.\n",
      "    bw: int, float or str\n",
      "        If numeric, indicates the bandwidth and must be positive.\n",
      "        If str, indicates the method to estimate the bandwidth.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    bw: float\n",
      "        Bandwidth\n",
      "    \"\"\"\n",
      "    if isinstance(bw, bool):\n",
      "        raise ValueError(\n",
      "            (\n",
      "                \"`bw` must not be of type `bool`.\\n\"\n",
      "                \"Expected a positive numeric or one of the following strings:\\n\"\n",
      "                f\"{list(_BW_METHODS_LINEAR)}.\"\n",
      "            )\n",
      "        )\n",
      "    if isinstance(bw, (int, float)):\n",
      "        if bw < 0:\n",
      "            raise ValueError(f\"Numeric `bw` must be positive.\\nInput: {bw:.4f}.\")\n",
      "    elif isinstance(bw, str):\n",
      "        bw_lower = bw.lower()\n",
      "\n",
      "        if bw_lower not in _BW_METHODS_LINEAR:\n",
      "            raise ValueError(\n",
      "                \"Unrecognized bandwidth method.\\n\"\n",
      "                f\"Input is: {bw_lower}.\\n\"\n",
      "                f\"Expected one of: {list(_BW_METHODS_LINEAR)}.\"\n",
      "            )\n",
      "\n",
      "        bw_fun = _BW_METHODS_LINEAR[bw_lower]\n",
      "        bw = bw_fun(x, grid_counts=grid_counts, x_std=x_std, x_range=x_range)\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"Unrecognized `bw` argument.\\n\"\n",
      "            \"Expected a positive numeric or one of the following strings:\\n\"\n",
      "            f\"{list(_BW_METHODS_LINEAR)}.\"\n",
      "        )\n",
      "    return bw\n",
      "\n",
      "\n",
      "def _vonmises_pdf(x, mu, kappa):\n",
      "    \"\"\"Calculate vonmises_pdf.\"\"\"\n",
      "    if kappa <= 0:\n",
      "        raise ValueError(\"Argument 'kappa' must be positive.\")\n",
      "    pdf = 1 / (2 * np.pi * ive(0, kappa)) * np.exp(np.cos(x - mu) - 1) ** kappa\n",
      "    return pdf\n",
      "\n",
      "\n",
      "def _a1inv(x):\n",
      "    \"\"\"Compute inverse function.\n",
      "\n",
      "    Inverse function of the ratio of the first and\n",
      "    zeroth order Bessel functions of the first kind.\n",
      "\n",
      "    Returns the value k, such that a1inv(x) = k, i.e. a1(k) = x.\n",
      "    \"\"\"\n",
      "    if 0 <= x < 0.53:\n",
      "        return 2 * x + x**3 + (5 * x**5) / 6\n",
      "    elif x < 0.85:\n",
      "        return -0.4 + 1.39 * x + 0.43 / (1 - x)\n",
      "    else:\n",
      "        return 1 / (x**3 - 4 * x**2 + 3 * x)\n",
      "\n",
      "\n",
      "def _kappa_mle(x):\n",
      "    mean = _circular_mean(x)\n",
      "    kappa = _a1inv(np.mean(np.cos(x - mean)))\n",
      "    return kappa\n",
      "\n",
      "\n",
      "def _dct1d(x):\n",
      "    \"\"\"Discrete Cosine Transform in 1 Dimension.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : numpy array\n",
      "        1 dimensional array of values for which the\n",
      "        DCT is desired\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    output : DTC transformed values\n",
      "    \"\"\"\n",
      "    x_len = len(x)\n",
      "\n",
      "    even_increasing = np.arange(0, x_len, 2)\n",
      "    odd_decreasing = np.arange(x_len - 1, 0, -2)\n",
      "\n",
      "    x = np.concatenate((x[even_increasing], x[odd_decreasing]))\n",
      "\n",
      "    w_1k = np.r_[1, (2 * np.exp(-(0 + 1j) * (np.arange(1, x_len)) * np.pi / (2 * x_len)))]\n",
      "    output = np.real(w_1k * fft(x))\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def _fixed_point(t, N, k_sq, a_sq):\n",
      "    \"\"\"Calculate t-zeta*gamma^[l](t).\n",
      "\n",
      "    Implementation of the function t-zeta*gamma^[l](t) derived from equation (30) in [1].\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Kernel density estimation via diffusion.\n",
      "       Z. I. Botev, J. F. Grotowski, and D. P. Kroese.\n",
      "       Ann. Statist. 38 (2010), no. 5, 2916--2957.\n",
      "    \"\"\"\n",
      "    k_sq = np.asfarray(k_sq, dtype=np.float64)\n",
      "    a_sq = np.asfarray(a_sq, dtype=np.float64)\n",
      "\n",
      "    l = 7\n",
      "    f = np.sum(np.power(k_sq, l) * a_sq * np.exp(-k_sq * np.pi**2 * t))\n",
      "    f *= 0.5 * np.pi ** (2.0 * l)\n",
      "\n",
      "    for j in np.arange(l - 1, 2 - 1, -1):\n",
      "        c1 = (1 + 0.5 ** (j + 0.5)) / 3\n",
      "        c2 = np.product(np.arange(1.0, 2 * j + 1, 2, dtype=np.float64))\n",
      "        c2 /= (np.pi / 2) ** 0.5\n",
      "        t_j = np.power((c1 * (c2 / (N * f))), (2.0 / (3.0 + 2.0 * j)))\n",
      "        f = np.sum(k_sq**j * a_sq * np.exp(-k_sq * np.pi**2.0 * t_j))\n",
      "        f *= 0.5 * np.pi ** (2 * j)\n",
      "\n",
      "    out = t - (2 * N * np.pi**0.5 * f) ** (-0.4)\n",
      "    return out\n",
      "\n",
      "\n",
      "def _root(function, N, args, x):\n",
      "    # The right bound is at most 0.01\n",
      "    found = False\n",
      "    N = max(min(1050, N), 50)\n",
      "    tol = 10e-12 + 0.01 * (N - 50) / 1000\n",
      "\n",
      "    while not found:\n",
      "        try:\n",
      "            bw, res = brentq(function, 0, 0.01, args=args, full_output=True, disp=False)\n",
      "            found = res.converged\n",
      "        except ValueError:\n",
      "            bw = 0\n",
      "            tol *= 2.0\n",
      "            found = False\n",
      "        if bw <= 0 or tol >= 1:\n",
      "            bw = (_bw_silverman(x) / np.ptp(x)) ** 2\n",
      "            return bw\n",
      "    return bw\n",
      "\n",
      "\n",
      "def _check_custom_lims(custom_lims, x_min, x_max):\n",
      "    \"\"\"Check if `custom_lims` are of the correct type.\n",
      "\n",
      "    It accepts numeric lists/tuples of length 2.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    custom_lims : Object whose type is checked.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    None: Object of type None\n",
      "    \"\"\"\n",
      "    if not isinstance(custom_lims, (list, tuple)):\n",
      "        raise TypeError(\n",
      "            \"`custom_lims` must be a numeric list or tuple of length 2.\\n\"\n",
      "            f\"Not an object of {type(custom_lims)}.\"\n",
      "        )\n",
      "\n",
      "    if len(custom_lims) != 2:\n",
      "        raise AttributeError(f\"`len(custom_lims)` must be 2, not {len(custom_lims)}.\")\n",
      "\n",
      "    any_bool = any(isinstance(i, bool) for i in custom_lims)\n",
      "    if any_bool:\n",
      "        raise TypeError(\"Elements of `custom_lims` must be numeric or None, not bool.\")\n",
      "\n",
      "    custom_lims = list(custom_lims)  # convert to a mutable object\n",
      "    if custom_lims[0] is None:\n",
      "        custom_lims[0] = x_min\n",
      "\n",
      "    if custom_lims[1] is None:\n",
      "        custom_lims[1] = x_max\n",
      "\n",
      "    all_numeric = all(isinstance(i, (int, float, np.integer, np.float)) for i in custom_lims)\n",
      "    if not all_numeric:\n",
      "        raise TypeError(\n",
      "            (\"Elements of `custom_lims` must be numeric or None.\\n\" \"At least one of them is not.\")\n",
      "        )\n",
      "\n",
      "    if not custom_lims[0] < custom_lims[1]:\n",
      "        raise ValueError(\"`custom_lims[0]` must be smaller than `custom_lims[1]`.\")\n",
      "\n",
      "    if custom_lims[0] > x_min or custom_lims[1] < x_max:\n",
      "        raise ValueError(\"Some observations are outside `custom_lims` boundaries.\")\n",
      "\n",
      "    return custom_lims\n",
      "\n",
      "\n",
      "def _get_grid(\n",
      "    x_min, x_max, x_std, extend_fct, grid_len, custom_lims, extend=True, bound_correction=False\n",
      "):\n",
      "    \"\"\"Compute the grid that bins the data used to estimate the density function.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x_min : float\n",
      "        Minimum value of the data\n",
      "    x_max: float\n",
      "        Maximum value of the data.\n",
      "    x_std: float\n",
      "        Standard deviation of the data.\n",
      "    extend_fct: bool\n",
      "        Indicates the factor by which `x_std` is multiplied\n",
      "        to extend the range of the data.\n",
      "    grid_len: int\n",
      "        Number of bins\n",
      "    custom_lims: tuple or list\n",
      "        Custom limits for the domain of the density estimation.\n",
      "        Must be numeric of length 2. Overrides `extend`.\n",
      "    extend: bool, optional\n",
      "        Whether to extend the range of the data or not.\n",
      "        Default is True.\n",
      "    bound_correction: bool, optional\n",
      "        Whether the density estimations performs boundary correction or not.\n",
      "        This does not impacts directly in the output, but is used\n",
      "        to override `extend`. Overrides `extend`.\n",
      "        Default is False.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    grid_len: int\n",
      "        Number of bins\n",
      "    grid_min: float\n",
      "        Minimum value of the grid\n",
      "    grid_max: float\n",
      "        Maximum value of the grid\n",
      "    \"\"\"\n",
      "    # Set up number of bins.\n",
      "    grid_len = max(int(grid_len), 100)\n",
      "\n",
      "    # Set up domain\n",
      "    if custom_lims is not None:\n",
      "        custom_lims = _check_custom_lims(custom_lims, x_min, x_max)\n",
      "        grid_min = custom_lims[0]\n",
      "        grid_max = custom_lims[1]\n",
      "    elif extend and not bound_correction:\n",
      "        grid_extend = extend_fct * x_std\n",
      "        grid_min = x_min - grid_extend\n",
      "        grid_max = x_max + grid_extend\n",
      "    else:\n",
      "        grid_min = x_min\n",
      "        grid_max = x_max\n",
      "    return grid_min, grid_max, grid_len\n",
      "\n",
      "\n",
      "def kde(x, circular=False, **kwargs):\n",
      "    \"\"\"One dimensional density estimation.\n",
      "\n",
      "    It is a wrapper around ``kde_linear()`` and ``kde_circular()``.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x: 1D numpy array\n",
      "        Data used to calculate the density estimation.\n",
      "    circular: bool, optional\n",
      "        Whether ``x`` is a circular variable or not. Defaults to False.\n",
      "    **kwargs\n",
      "        Arguments passed to ``kde_linear()`` and ``kde_circular()``.\n",
      "        See their documentation for more info.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    grid: Gridded numpy array for the x values.\n",
      "    pdf: Numpy array for the density estimates.\n",
      "    bw: optional, the estimated bandwidth.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Default density estimation for linear data\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from arviz import kde\n",
      "        >>>\n",
      "        >>> rvs = np.random.gamma(shape=1.8, size=1000)\n",
      "        >>> grid, pdf = kde(rvs)\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Density estimation for linear data with Silverman's rule bandwidth\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> grid, pdf = kde(rvs, bw=\"silverman\")\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Density estimation for linear data with scaled bandwidth\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> # bw_fct > 1 means more smoothness.\n",
      "        >>> grid, pdf = kde(rvs, bw_fct=2.5)\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Default density estimation for linear data with extended limits\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> grid, pdf = kde(rvs, bound_correction=False, extend=True, extend_fct=0.5)\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Default density estimation for linear data with custom limits\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> # It accepts tuples and lists of length 2.\n",
      "        >>> grid, pdf = kde(rvs, bound_correction=False, custom_lims=(0, 10))\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Default density estimation for circular data\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> rvs = np.random.vonmises(mu=np.pi, kappa=1, size=500)\n",
      "        >>> grid, pdf = kde(rvs, circular=True)\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Density estimation for circular data with scaled bandwidth\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> rvs = np.random.vonmises(mu=np.pi, kappa=1, size=500)\n",
      "        >>> # bw_fct > 1 means less smoothness.\n",
      "        >>> grid, pdf = kde(rvs, circular=True, bw_fct=3)\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    Density estimation for circular data with custom limits\n",
      "\n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "\n",
      "        >>> # This is still experimental, does not always work.\n",
      "        >>> rvs = np.random.vonmises(mu=0, kappa=30, size=500)\n",
      "        >>> grid, pdf = kde(rvs, circular=True, custom_lims=(-1, 1))\n",
      "        >>> plt.plot(grid, pdf)\n",
      "        >>> plt.show()\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    plot_kde : Compute and plot a kernel density estimate.\n",
      "    \"\"\"\n",
      "    x = x[np.isfinite(x)]\n",
      "    if x.size == 0 or np.all(x == x[0]):\n",
      "        warnings.warn(\"Your data appears to have a single value or no finite values\")\n",
      "\n",
      "        return np.zeros(2), np.array([np.nan] * 2)\n",
      "\n",
      "    if circular:\n",
      "        if circular == \"degrees\":\n",
      "            x = np.radians(x)\n",
      "        kde_fun = _kde_circular\n",
      "    else:\n",
      "        kde_fun = _kde_linear\n",
      "\n",
      "    return kde_fun(x, **kwargs)\n",
      "\n",
      "\n",
      "def _kde_linear(\n",
      "    x,\n",
      "    bw=\"experimental\",\n",
      "    adaptive=False,\n",
      "    extend=False,\n",
      "    bound_correction=True,\n",
      "    extend_fct=0,\n",
      "    bw_fct=1,\n",
      "    bw_return=False,\n",
      "    custom_lims=None,\n",
      "    cumulative=False,\n",
      "    grid_len=512,\n",
      "    **kwargs,  # pylint: disable=unused-argument\n",
      "):\n",
      "    \"\"\"One dimensional density estimation for linear data.\n",
      "\n",
      "    Given an array of data points `x` it returns an estimate of\n",
      "    the probability density function that generated the samples in `x`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : 1D numpy array\n",
      "        Data used to calculate the density estimation.\n",
      "    bw: int, float or str, optional\n",
      "        If numeric, indicates the bandwidth and must be positive.\n",
      "        If str, indicates the method to estimate the bandwidth and must be one of \"scott\",\n",
      "        \"silverman\", \"isj\" or \"experimental\". Defaults to \"experimental\".\n",
      "    adaptive: boolean, optional\n",
      "        Indicates if the bandwidth is adaptive or not.\n",
      "        It is the recommended approach when there are multiple modes with different spread.\n",
      "        It is not compatible with convolution. Defaults to False.\n",
      "    extend: boolean, optional\n",
      "        Whether to extend the observed range for `x` in the estimation.\n",
      "        It extends each bound by a multiple of the standard deviation of `x` given by `extend_fct`.\n",
      "        Defaults to False.\n",
      "    bound_correction: boolean, optional\n",
      "        Whether to perform boundary correction on the bounds of `x` or not.\n",
      "        Defaults to True.\n",
      "    extend_fct: float, optional\n",
      "        Number of standard deviations used to widen the lower and upper bounds of `x`.\n",
      "        Defaults to 0.5.\n",
      "    bw_fct: float, optional\n",
      "        A value that multiplies `bw` which enables tuning smoothness by hand.\n",
      "        Must be positive. Values below 1 decrease smoothness while values above 1 decrease it.\n",
      "        Defaults to 1 (no modification).\n",
      "    bw_return: bool, optional\n",
      "        Whether to return the estimated bandwidth in addition to the other objects.\n",
      "        Defaults to False.\n",
      "    custom_lims: list or tuple, optional\n",
      "        A list or tuple of length 2 indicating custom bounds for the range of `x`.\n",
      "        Defaults to None which disables custom bounds.\n",
      "    cumulative: bool, optional\n",
      "        Whether return the PDF or the cumulative PDF. Defaults to False.\n",
      "    grid_len: int, optional\n",
      "        The number of intervals used to bin the data points i.e. the length of the grid used in\n",
      "        the estimation. Defaults to 512.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    grid : Gridded numpy array for the x values.\n",
      "    pdf : Numpy array for the density estimates.\n",
      "    bw: optional, the estimated bandwidth.\n",
      "    \"\"\"\n",
      "    # Check `bw_fct` is numeric and positive\n",
      "    if not isinstance(bw_fct, (int, float, np.integer, np.floating)):\n",
      "        raise TypeError(f\"`bw_fct` must be a positive number, not an object of {type(bw_fct)}.\")\n",
      "\n",
      "    if bw_fct <= 0:\n",
      "        raise ValueError(f\"`bw_fct` must be a positive number, not {bw_fct}.\")\n",
      "\n",
      "    # Preliminary calculations\n",
      "    x_min = x.min()\n",
      "    x_max = x.max()\n",
      "    x_std = np.std(x)\n",
      "    x_range = x_max - x_min\n",
      "\n",
      "    # Determine grid\n",
      "    grid_min, grid_max, grid_len = _get_grid(\n",
      "        x_min, x_max, x_std, extend_fct, grid_len, custom_lims, extend, bound_correction\n",
      "    )\n",
      "    grid_counts, _, grid_edges = histogram(x, grid_len, (grid_min, grid_max))\n",
      "\n",
      "    # Bandwidth estimation\n",
      "    bw = bw_fct * _get_bw(x, bw, grid_counts, x_std, x_range)\n",
      "\n",
      "    # Density estimation\n",
      "    if adaptive:\n",
      "        grid, pdf = _kde_adaptive(x, bw, grid_edges, grid_counts, grid_len, bound_correction)\n",
      "    else:\n",
      "        grid, pdf = _kde_convolution(x, bw, grid_edges, grid_counts, grid_len, bound_correction)\n",
      "\n",
      "    if cumulative:\n",
      "        pdf = pdf.cumsum() / pdf.sum()\n",
      "\n",
      "    if bw_return:\n",
      "        return grid, pdf, bw\n",
      "    else:\n",
      "        return grid, pdf\n",
      "\n",
      "\n",
      "def _kde_circular(\n",
      "    x,\n",
      "    bw=\"taylor\",\n",
      "    bw_fct=1,\n",
      "    bw_return=False,\n",
      "    custom_lims=None,\n",
      "    cumulative=False,\n",
      "    grid_len=512,\n",
      "    **kwargs,  # pylint: disable=unused-argument\n",
      "):\n",
      "    \"\"\"One dimensional density estimation for circular data.\n",
      "\n",
      "    Given an array of data points `x` measured in radians, it returns an estimate of the\n",
      "    probability density function that generated the samples in `x`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : 1D numpy array\n",
      "        Data used to calculate the density estimation.\n",
      "    bw: int, float or str, optional\n",
      "        If numeric, indicates the bandwidth and must be positive.\n",
      "        If str, indicates the method to estimate the bandwidth and must be \"taylor\" since it is the\n",
      "        only option supported so far. Defaults to \"taylor\".\n",
      "    bw_fct: float, optional\n",
      "        A value that multiplies `bw` which enables tuning smoothness by hand. Must be positive.\n",
      "        Values above 1 decrease smoothness while values below 1 decrease it.\n",
      "        Defaults to 1 (no modification).\n",
      "    bw_return: bool, optional\n",
      "        Whether to return the estimated bandwidth in addition to the other objects.\n",
      "        Defaults to False.\n",
      "    custom_lims: list or tuple, optional\n",
      "        A list or tuple of length 2 indicating custom bounds for the range of `x`.\n",
      "        Defaults to None which means the estimation limits are [-pi, pi].\n",
      "    cumulative: bool, optional\n",
      "        Whether return the PDF or the cumulative PDF. Defaults to False.\n",
      "    grid_len: int, optional\n",
      "        The number of intervals used to bin the data pointa i.e. the length of the grid used in the\n",
      "        estimation. Defaults to 512.\n",
      "    \"\"\"\n",
      "    # All values between -pi and pi\n",
      "    x = _normalize_angle(x)\n",
      "\n",
      "    # Check `bw_fct` is numeric and positive\n",
      "    if not isinstance(bw_fct, (int, float, np.integer, np.floating)):\n",
      "        raise TypeError(f\"`bw_fct` must be a positive number, not an object of {type(bw_fct)}.\")\n",
      "\n",
      "    if bw_fct <= 0:\n",
      "        raise ValueError(f\"`bw_fct` must be a positive number, not {bw_fct}.\")\n",
      "\n",
      "    # Determine bandwidth\n",
      "    if isinstance(bw, bool):\n",
      "        raise ValueError(\n",
      "            \"`bw` can't be of type `bool`.\\n\" \"Expected a positive numeric or 'taylor'\"\n",
      "        )\n",
      "    if isinstance(bw, (int, float)):\n",
      "        if bw < 0:\n",
      "            raise ValueError(f\"Numeric `bw` must be positive.\\nInput: {bw:.4f}.\")\n",
      "    if isinstance(bw, str):\n",
      "        if bw == \"taylor\":\n",
      "            bw = _bw_taylor(x)\n",
      "        else:\n",
      "            raise ValueError(f\"`bw` must be a positive numeric or `taylor`, not {bw}\")\n",
      "    bw *= bw_fct\n",
      "\n",
      "    # Determine grid\n",
      "    if custom_lims is not None:\n",
      "        custom_lims = _check_custom_lims(custom_lims, x.min(), x.max())\n",
      "        grid_min = custom_lims[0]\n",
      "        grid_max = custom_lims[1]\n",
      "        assert grid_min >= -np.pi, \"Lower limit can't be smaller than -pi\"\n",
      "        assert grid_max <= np.pi, \"Upper limit can't be larger than pi\"\n",
      "    else:\n",
      "        grid_min = -np.pi\n",
      "        grid_max = np.pi\n",
      "\n",
      "    bins = np.linspace(grid_min, grid_max, grid_len + 1)\n",
      "    bin_counts, _, bin_edges = histogram(x, bins=bins)\n",
      "    grid = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
      "\n",
      "    kern = _vonmises_pdf(x=grid, mu=0, kappa=bw)\n",
      "    pdf = np.fft.fftshift(np.fft.irfft(np.fft.rfft(kern) * np.fft.rfft(bin_counts)))\n",
      "    pdf /= len(x)\n",
      "\n",
      "    if cumulative:\n",
      "        pdf = pdf.cumsum() / pdf.sum()\n",
      "\n",
      "    if bw_return:\n",
      "        return grid, pdf, bw\n",
      "    else:\n",
      "        return grid, pdf\n",
      "\n",
      "\n",
      "# pylint: disable=unused-argument\n",
      "def _kde_convolution(x, bw, grid_edges, grid_counts, grid_len, bound_correction, **kwargs):\n",
      "    \"\"\"Kernel density with convolution.\n",
      "\n",
      "    One dimensional Gaussian kernel density estimation via convolution of the binned relative\n",
      "    frequencies and a Gaussian filter. This is an internal function used by `kde()`.\n",
      "    \"\"\"\n",
      "    # Calculate relative frequencies per bin\n",
      "    bin_width = grid_edges[1] - grid_edges[0]\n",
      "    f = grid_counts / bin_width / len(x)\n",
      "\n",
      "    # Bandwidth must consider the bin width\n",
      "    bw /= bin_width\n",
      "\n",
      "    # See: https://stackoverflow.com/questions/2773606/gaussian-filter-in-matlab\n",
      "\n",
      "    grid = (grid_edges[1:] + grid_edges[:-1]) / 2\n",
      "\n",
      "    kernel_n = int(bw * 2 * np.pi)\n",
      "    if kernel_n == 0:\n",
      "        kernel_n = 1\n",
      "\n",
      "    kernel = gaussian(kernel_n, bw)\n",
      "\n",
      "    if bound_correction:\n",
      "        npad = int(grid_len / 5)\n",
      "        f = np.concatenate([f[npad - 1 :: -1], f, f[grid_len : grid_len - npad - 1 : -1]])\n",
      "        pdf = convolve(f, kernel, mode=\"same\", method=\"direct\")[npad : npad + grid_len]\n",
      "        pdf /= bw * (2 * np.pi) ** 0.5\n",
      "    else:\n",
      "        pdf = convolve(f, kernel, mode=\"same\", method=\"direct\")\n",
      "        pdf /= bw * (2 * np.pi) ** 0.5\n",
      "\n",
      "    return grid, pdf\n",
      "\n",
      "\n",
      "def _kde_adaptive(x, bw, grid_edges, grid_counts, grid_len, bound_correction, **kwargs):\n",
      "    \"\"\"Compute Adaptive Kernel Density Estimation.\n",
      "\n",
      "    One dimensional adaptive Gaussian kernel density estimation. The implementation uses the binning\n",
      "    technique. Since there is not an unique `bw`, the convolution is not possible. The alternative\n",
      "    implemented in this function is known as Abramson's method.\n",
      "    This is an internal function used by `kde()`.\n",
      "    \"\"\"\n",
      "    # Pilot computations used for bandwidth adjustment\n",
      "    pilot_grid, pilot_pdf = _kde_convolution(\n",
      "        x, bw, grid_edges, grid_counts, grid_len, bound_correction\n",
      "    )\n",
      "\n",
      "    # Adds to avoid np.log(0) and zero division\n",
      "    pilot_pdf += 1e-9\n",
      "\n",
      "    # Determine the modification factors\n",
      "    pdf_interp = np.interp(x, pilot_grid, pilot_pdf)\n",
      "    geom_mean = np.exp(np.mean(np.log(pdf_interp)))\n",
      "\n",
      "    # Power of c = 0.5 -> Abramson's method\n",
      "    adj_factor = (geom_mean / pilot_pdf) ** 0.5\n",
      "    bw_adj = bw * adj_factor\n",
      "\n",
      "    # Estimation of Gaussian KDE via binned method (convolution not possible)\n",
      "    grid = pilot_grid\n",
      "\n",
      "    if bound_correction:\n",
      "        grid_npad = int(grid_len / 5)\n",
      "        grid_width = grid_edges[1] - grid_edges[0]\n",
      "        grid_pad = grid_npad * grid_width\n",
      "        grid_padded = np.linspace(\n",
      "            grid_edges[0] - grid_pad,\n",
      "            grid_edges[grid_len - 1] + grid_pad,\n",
      "            num=grid_len + 2 * grid_npad,\n",
      "        )\n",
      "        grid_counts = np.concatenate(\n",
      "            [\n",
      "                grid_counts[grid_npad - 1 :: -1],\n",
      "                grid_counts,\n",
      "                grid_counts[grid_len : grid_len - grid_npad - 1 : -1],\n",
      "            ]\n",
      "        )\n",
      "        bw_adj = np.concatenate(\n",
      "            [bw_adj[grid_npad - 1 :: -1], bw_adj, bw_adj[grid_len : grid_len - grid_npad - 1 : -1]]\n",
      "        )\n",
      "        pdf_mat = (grid_padded - grid_padded[:, None]) / bw_adj[:, None]\n",
      "        pdf_mat = np.exp(-0.5 * pdf_mat**2) * grid_counts[:, None]\n",
      "        pdf_mat /= (2 * np.pi) ** 0.5 * bw_adj[:, None]\n",
      "        pdf = np.sum(pdf_mat[:, grid_npad : grid_npad + grid_len], axis=0) / len(x)\n",
      "\n",
      "    else:\n",
      "        pdf_mat = (grid - grid[:, None]) / bw_adj[:, None]\n",
      "        pdf_mat = np.exp(-0.5 * pdf_mat**2) * grid_counts[:, None]\n",
      "        pdf_mat /= (2 * np.pi) ** 0.5 * bw_adj[:, None]\n",
      "        pdf = np.sum(pdf_mat, axis=0) / len(x)\n",
      "\n",
      "    return grid, pdf\n",
      "\n",
      "\n",
      "def _fast_kde_2d(x, y, gridsize=(128, 128), circular=False):\n",
      "    \"\"\"\n",
      "    2D fft-based Gaussian kernel density estimate (KDE).\n",
      "\n",
      "    The code was adapted from https://github.com/mfouesneau/faststats\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : Numpy array or list\n",
      "    y : Numpy array or list\n",
      "    gridsize : tuple\n",
      "        Number of points used to discretize data. Use powers of 2 for fft optimization\n",
      "    circular: bool\n",
      "        If True use circular boundaries. Defaults to False\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    grid: A gridded 2D KDE of the input points (x, y)\n",
      "    xmin: minimum value of x\n",
      "    xmax: maximum value of x\n",
      "    ymin: minimum value of y\n",
      "    ymax: maximum value of y\n",
      "    \"\"\"\n",
      "    x = np.asarray(x, dtype=float)\n",
      "    x = x[np.isfinite(x)]\n",
      "    y = np.asarray(y, dtype=float)\n",
      "    y = y[np.isfinite(y)]\n",
      "\n",
      "    xmin, xmax = x.min(), x.max()\n",
      "    ymin, ymax = y.min(), y.max()\n",
      "\n",
      "    len_x = len(x)\n",
      "    weights = np.ones(len_x)\n",
      "    n_x, n_y = gridsize\n",
      "\n",
      "    d_x = (xmax - xmin) / (n_x - 1)\n",
      "    d_y = (ymax - ymin) / (n_y - 1)\n",
      "\n",
      "    xyi = _stack(x, y).T\n",
      "    xyi -= [xmin, ymin]\n",
      "    xyi /= [d_x, d_y]\n",
      "    xyi = np.floor(xyi, xyi).T\n",
      "\n",
      "    scotts_factor = len_x ** (-1 / 6)\n",
      "    cov = _cov(xyi)\n",
      "    std_devs = np.diag(cov) ** 0.5\n",
      "    kern_nx, kern_ny = np.round(scotts_factor * 2 * np.pi * std_devs)\n",
      "\n",
      "    inv_cov = np.linalg.inv(cov * scotts_factor**2)\n",
      "\n",
      "    x_x = np.arange(kern_nx) - kern_nx / 2\n",
      "    y_y = np.arange(kern_ny) - kern_ny / 2\n",
      "    x_x, y_y = np.meshgrid(x_x, y_y)\n",
      "\n",
      "    kernel = _stack(x_x.flatten(), y_y.flatten())\n",
      "    kernel = _dot(inv_cov, kernel) * kernel\n",
      "    kernel = np.exp(-kernel.sum(axis=0) / 2)\n",
      "    kernel = kernel.reshape((int(kern_ny), int(kern_nx)))\n",
      "\n",
      "    boundary = \"wrap\" if circular else \"symm\"\n",
      "\n",
      "    grid = coo_matrix((weights, xyi), shape=(n_x, n_y)).toarray()\n",
      "    grid = convolve2d(grid, kernel, mode=\"same\", boundary=boundary)\n",
      "\n",
      "    norm_factor = np.linalg.det(2 * np.pi * cov * scotts_factor**2)\n",
      "    norm_factor = len_x * d_x * d_y * norm_factor**0.5\n",
      "\n",
      "    grid /= norm_factor\n",
      "\n",
      "    return grid, xmin, xmax, ymin, ymax\n",
      "\n",
      "\n",
      "def get_bins(values):\n",
      "    \"\"\"\n",
      "    Automatically compute the number of bins for discrete variables.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    values = numpy array\n",
      "        values\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    array with the bins\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Computes the width of the bins by taking the maximum of the Sturges and the Freedman-Diaconis\n",
      "    estimators. According to numpy `np.histogram` this provides good all around performance.\n",
      "\n",
      "    The Sturges is a very simplistic estimator based on the assumption of normality of the data.\n",
      "    This estimator has poor performance for non-normal data, which becomes especially obvious for\n",
      "    large data sets. The estimate depends only on size of the data.\n",
      "\n",
      "    The Freedman-Diaconis rule uses interquartile range (IQR) to estimate the binwidth.\n",
      "    It is considered a robust version of the Scott rule as the IQR is less affected by outliers\n",
      "    than the standard deviation. However, the IQR depends on fewer points than the standard\n",
      "    deviation, so it is less accurate, especially for long tailed distributions.\n",
      "    \"\"\"\n",
      "    dtype = values.dtype.kind\n",
      "\n",
      "    if dtype == \"i\":\n",
      "        x_min = values.min().astype(int)\n",
      "        x_max = values.max().astype(int)\n",
      "    else:\n",
      "        x_min = values.min().astype(float)\n",
      "        x_max = values.max().astype(float)\n",
      "\n",
      "    # Sturges histogram bin estimator\n",
      "    bins_sturges = (x_max - x_min) / (np.log2(values.size) + 1)\n",
      "\n",
      "    # The Freedman-Diaconis histogram bin estimator.\n",
      "    iqr = np.subtract(*np.percentile(values, [75, 25]))  # pylint: disable=assignment-from-no-return\n",
      "    bins_fd = 2 * iqr * values.size ** (-1 / 3)\n",
      "\n",
      "    if dtype == \"i\":\n",
      "        width = np.round(np.max([1, bins_sturges, bins_fd])).astype(int)\n",
      "        bins = np.arange(x_min, x_max + width + 1, width)\n",
      "    else:\n",
      "        width = np.max([bins_sturges, bins_fd])\n",
      "        if np.isclose(x_min, x_max):\n",
      "            width = 1e-3\n",
      "        bins = np.arange(x_min, x_max + width, width)\n",
      "\n",
      "    return bins\n",
      "\n",
      "\n",
      "def _sturges_formula(dataset, mult=1):\n",
      "    \"\"\"Use Sturges' formula to determine number of bins.\n",
      "\n",
      "    See https://en.wikipedia.org/wiki/Histogram#Sturges'_formula\n",
      "    or https://doi.org/10.1080%2F01621459.1926.10502161\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset: xarray.DataSet\n",
      "       Must have the `draw` dimension\n",
      "\n",
      "    mult: float\n",
      "        Used to scale the number of bins up or down. Default is 1 for Sturges' formula.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    int\n",
      "        Number of bins to use\n",
      "    \"\"\"\n",
      "    return int(np.ceil(mult * np.log2(dataset.draw.size)) + 1)\n",
      "\n",
      "\n",
      "def _circular_mean(x):\n",
      "    \"\"\"Compute mean of circular variable measured in radians.\n",
      "\n",
      "    The result is between -pi and pi.\n",
      "    \"\"\"\n",
      "    sinr = np.sum(np.sin(x))\n",
      "    cosr = np.sum(np.cos(x))\n",
      "    mean = np.arctan2(sinr, cosr)\n",
      "\n",
      "    return mean\n",
      "\n",
      "\n",
      "def _normalize_angle(x, zero_centered=True):\n",
      "    \"\"\"Normalize angles.\n",
      "\n",
      "    Normalize angles in radians to [-pi, pi) or [0, 2 * pi) according to `zero_centered`.\n",
      "    \"\"\"\n",
      "    if zero_centered:\n",
      "        return (x + np.pi) % (2 * np.pi) - np.pi\n",
      "    else:\n",
      "        return x % (2 * np.pi)\n",
      "\n",
      "\n",
      "@conditional_jit(cache=True)\n",
      "def histogram(data, bins, range_hist=None):\n",
      "    \"\"\"Conditionally jitted histogram.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data : array-like\n",
      "        Input data. Passed as first positional argument to ``np.histogram``.\n",
      "    bins : int or array-like\n",
      "        Passed as keyword argument ``bins`` to ``np.histogram``.\n",
      "    range_hist : (float, float), optional\n",
      "        Passed as keyword argument ``range`` to ``np.histogram``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    hist : array\n",
      "        The number of counts per bin.\n",
      "    density : array\n",
      "        The density corresponding to each bin.\n",
      "    bin_edges : array\n",
      "        The edges of the bins used.\n",
      "    \"\"\"\n",
      "    hist, bin_edges = np.histogram(data, bins=bins, range=range_hist)\n",
      "    hist_dens = hist / (hist.sum() * np.diff(bin_edges))\n",
      "    return hist, hist_dens, bin_edges\n",
      "\n",
      "\n",
      "def _find_hdi_contours(density, hdi_probs):\n",
      "    \"\"\"\n",
      "    Find contours enclosing regions of highest posterior density.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    density : array-like\n",
      "        A 2D KDE on a grid with cells of equal area.\n",
      "    hdi_probs : array-like\n",
      "        An array of highest density interval confidence probabilities.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    contour_levels : array\n",
      "        The contour levels corresponding to the given HDI probabilities.\n",
      "    \"\"\"\n",
      "    # Using the algorithm from corner.py\n",
      "    sorted_density = np.sort(density, axis=None)[::-1]\n",
      "    sm = sorted_density.cumsum()\n",
      "    sm /= sm[-1]\n",
      "\n",
      "    contours = np.empty_like(hdi_probs)\n",
      "    for idx, hdi_prob in enumerate(hdi_probs):\n",
      "        try:\n",
      "            contours[idx] = sorted_density[sm <= hdi_prob][-1]\n",
      "        except IndexError:\n",
      "            contours[idx] = sorted_density[0]\n",
      "\n",
      "    return contours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(az.stats.density_utils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "93506df3-25fb-49cb-b654-5704041c61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function loo in module arviz.stats.stats:\n",
      "\n",
      "loo(data, pointwise=None, var_name=None, reff=None, scale=None)\n",
      "    Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n",
      "    \n",
      "    Estimates the expected log pointwise predictive density (elpd) using Pareto-smoothed\n",
      "    importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO's\n",
      "    standard error and the effective number of parameters. Read more theory here\n",
      "    https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data: obj\n",
      "        Any object that can be converted to an :class:`arviz.InferenceData` object.\n",
      "        Refer to documentation of\n",
      "        :func:`arviz.convert_to_dataset` for details.\n",
      "    pointwise: bool, optional\n",
      "        If True the pointwise predictive accuracy will be returned. Defaults to\n",
      "        ``stats.ic_pointwise`` rcParam.\n",
      "    var_name : str, optional\n",
      "        The name of the variable in log_likelihood groups storing the pointwise log\n",
      "        likelihood data to use for loo computation.\n",
      "    reff: float, optional\n",
      "        Relative MCMC efficiency, ``ess / n`` i.e. number of effective samples divided by the number\n",
      "        of actual samples. Computed from trace by default.\n",
      "    scale: str\n",
      "        Output scale for loo. Available options are:\n",
      "    \n",
      "        - ``log`` : (default) log-score\n",
      "        - ``negative_log`` : -1 * log-score\n",
      "        - ``deviance`` : -2 * log-score\n",
      "    \n",
      "        A higher log-score (or a lower deviance or negative log_score) indicates a model with\n",
      "        better predictive accuracy.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ELPDData object (inherits from :class:`pandas.Series`) with the following row/attributes:\n",
      "    loo: approximated expected log pointwise predictive density (elpd)\n",
      "    loo_se: standard error of loo\n",
      "    p_loo: effective number of parameters\n",
      "    shape_warn: bool\n",
      "        True if the estimated shape parameter of\n",
      "        Pareto distribution is greater than 0.7 for one or more samples\n",
      "    loo_i: array of pointwise predictive accuracy, only if pointwise True\n",
      "    pareto_k: array of Pareto shape values, only if pointwise True\n",
      "    loo_scale: scale of the loo results\n",
      "    \n",
      "        The returned object has a custom print method that overrides pd.Series method.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n",
      "    waic : Compute the widely applicable information criterion.\n",
      "    plot_compare : Summary plot for model comparison.\n",
      "    plot_elpd : Plot pointwise elpd differences between two or more models.\n",
      "    plot_khat : Plot Pareto tail indices for diagnosing convergence.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Calculate LOO of a model:\n",
      "    \n",
      "    .. ipython::\n",
      "    \n",
      "        In [1]: import arviz as az\n",
      "           ...: data = az.load_arviz_data(\"centered_eight\")\n",
      "           ...: az.loo(data)\n",
      "    \n",
      "    Calculate LOO of a model and return the pointwise values:\n",
      "    \n",
      "    .. ipython::\n",
      "    \n",
      "        In [2]: data_loo = az.loo(data, pointwise=True)\n",
      "           ...: data_loo.loo_i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.loo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argumentative_language",
   "language": "python",
   "name": "argumentative_language"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
